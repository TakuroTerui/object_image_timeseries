# 物体・画像認識と時系列データ処理入門 TensorFlow/Keras/TFLearnによる実装ディープラーニング
## 1章 ディープラーニングとは
### 1.1 深層学習（ディープラーニング）とは
#### 1.1.1 機械学習の活用例
#### 1.1.2 機械学習とディープラーニングの関係
- ■機械学習の主な手法
### 1.2 ディープラーニングって具体的に何をするの？
#### 1.2.1 ニューラルネットワークのニューロン
- ■人工ニューロン
- ■学習するということは重み・バイアスを適切な値に更新すること
- ■ニューロンの発火を司る「ステップ関数」
- ■ニューロンの発火を司る「シグモイド関数」
- ■人工ニューロンをネットワーク状に連結する
#### 1.2.2 ディープニューラルネットがディープラーニングを実現する
- ■畳み込みニューラルネットワーク（CNN）
- ■RNN（Recurrent Neural Network：再帰型ニューラルネットワーク）
- ■ドロップアウト、正則化
#### 1.2.3 ディープラーニングのためのライブラリ
#### 1.2.4 ディープラーニングの仕組み
#### 1.2.5 この本で使用するディープラーニング用データセット

## 2章 開発環境のセットアップとPython/TensorFlowの基礎
### 2.1 Anacondaの導入
#### 2.1.1 Anacondaのダウンロードとインストール
### 2.2 仮想環境の構築とライブラリのインストール
#### 2.2.1 専用の仮想環境を構築する
#### 2.2.2 ライブラリのインストール
#### 2.2.3 TFLearn用の仮想環境を作成して必要なライブラリをインストールする
- COLUMN pipのコマンド
### 2.3 Jupyter Notebookの使い方
#### 2.3.1 Jupyter Notebookを仮想環境にインストールする
#### 2.3.2 Notebookを作成する
- ■Notebookを保存するためのフォルダーを作成する
- ■Notebookの作成
#### 2.3.3 ソースコードを入力して実行する
- ■Jupyter Notebookのコマンド
### 2.4 Pythonの演算処理
#### 2.4.1 変数を使って演算する
#### 2.4.2 Pythonが扱うデータの種類
- ■ソースコードに説明文を書く
- COLUMN リストの中に要素製造装置を入れる（内包表記）・
### 2.5 Pythonのリスト
#### 2.5.1 リストを作る
### 2.6 if文とfor文
#### 2.6.1 if文
#### 2.6.2 条件式を作るための「比較演算子」
#### 2.6.3 if...elseで処理を分ける
#### 2.6.4 for文
### 2.7 関数
#### 2.7.1 処理だけを行う関数
#### 2.7.2 引数を受け取る関数
#### 2.7.3 処理結果を返す関数
### 2.8 クラス
#### 2.8.1 メソッド
#### 2.8.2 オリジナルのクラスを作る
- ■オブジェクトを作成する（クラスのインスタンス化）
#### 2.8.3 オブジェクトの初期化を行う__init__()
#### 2.8.4 インスタンスごとの情報を保持するインスタンス変数
- COLUMN リストの中に要素製造装置を入れる（内包表記）・
### 2.9 TensorFlowの使い方のキホン
#### 2.9.1 データフローグラフの作成
- COLUMN リストの中に要素製造装置を入れる（内包表記）・
- ■オペレーションノード（opノード）
- ■データフローグラフを使うメリット
#### 2.9.2 データフローグラフの実行
- ■セッションの流れを追う
#### 2.9.3 データフローグラフを実行するいくつかのパターン
- ■global_variables_initializer()で変数をまとめて初期化する
- ■セッションをインタラクティブモードで実行する
#### 2.9.4 with文でセッション全体をラップする
- ■セッションをwith文にまとめるメリット
- ■with文でセッション全体をラップする
- COLUMN TensorFlowの演算子
#### 2.9.5 tf.placeholder（プレースホルダー）
- ■実行時に確定する値をプレースホルダーで保持する

## 3章 ディープラーニングの数学的要素
### 3.1 ニューラルネットワークのデータ表現：テンソル
#### 3.1.1 NumPyのスカラー（0階テンソル）
#### 3.1.2 NumPyのベクトル（1階テンソル）
#### 3.1.3 NumPyの行列（2階テンソル）
#### 3.1.4 3階テンソルとより高階数のテンソル
### 3.2 ニューラルネットワークを回す（ベクトルの演算）
#### 3.2.1 ベクトルの算術演算
#### 3.2.2 ベクトルのスカラー演算
#### 3.2.3 ベクトル同士の四則演算
- ■ベクトル同士の加算と減算
#### 3.2.4 ベクトルのアダマール積を求める
#### 3.2.5 ベクトルの内積を求める
### 3.3 ニューラルネットワークを回す（行列の演算）
#### 3.3.1 行列の構造
#### 3.3.2 多次元配列で行列を表現する
#### 3.3.3 行列のスカラー演算
#### 3.3.4 行列の定数倍
#### 3.3.5 行列の成分にアクセスする
#### 3.3.6 行列の成分同士を加算、減算する
#### 3.3.7 行列のアダマール積
#### 3.3.8 行列の内積を求める
- ■行列同士の内積を求めてみる
#### 3.3.9 行と列を入れ替えて「転置行列」を作る
### 3.4 微分
#### 3.4.1 極限（lim）
#### 3.4.2 微分の基礎
#### 3.4.3 微分をPythonで実装してみる
- ■数値微分で関数を微分してみる
- ■プログラムの実行結果
#### 3.4.4 微分の公式
#### 3.4.5 変数が2つ以上の場合の微分（偏微分）
#### 3.4.6 合成関数の微分
- ■合成関数のチェーンルールの拡張
- ■積の微分法

## 4章 ニューラルネットワークの可動部（勾配ベースの最適化）
### 4.1 ロジスティック回帰を実装した単純パーセプトロンで二値分類を行う
#### 4.1.1 活性化関数による「発火」の仕組み
#### 4.1.2 シグモイド関数
- ■シグモイド関数（ロジスティック関数）
- ■シグモイド関数の実装
#### 4.1.3 シグモイド関数を活性化関数にしてパラメーターを最適化する
- ■最尤推定法と尤度関数
- ■対数尤度関数を微分しやすいように両端に対数をとる
- ■対数尤度関数を変形して交差エントロピー誤差関数にする
- ■対数尤度関数の微分
- COLUMN シグモイド関数の微分
- ■パラメーターの更新式の導出
#### 4.1.4 勾配降下法の考え方
- ■学習率の設定
- ■勾配降下法の更新式
#### 4.1.5 パーセプトロンで論理ゲートを実現する
- ■Pythonの標準仕様で論理ゲートを実装する
- ■TensorFlowで論理ゲートを実装する
- ■重みとバイアスの変数
- ■プレースホルダー
- ■operationを専門に行うopノード
- ■ミニマイザーを追加してデータフローグラフを完成する
- ■データフローグラフまでを完成させる
- ■セッション部の作成
- ■変数の初期化と実行
#### 4.1.6 Kerasで論理ゲートを実装する
- ■モデルの作成
- ■モデルの学習
- ■学習結果の確認
#### 4.1.7 TFLearnで論理ゲートを実装する

## 5章 ニューラルネットワーク
### 5.1 XORゲートを多層パーセプトロンで実現する
#### 5.1.1 多層パーセプトロンによるXORゲートの実現
- ■XORゲートは線形分離不可能な二値分類を行う
- ■多層パーセプトロンの構造
#### 5.1.2 TensorFlowによるXORゲートの実装
- ■入力層の作成（データフローグラフ）
- ■隠れ層（第1層）の作成（データフローグラフ）
- ■出力層（第2層）の作成（データフローグラフ）
- ■ミニマイザーを追加してデータフローグラフを完成する
- ■セッション部の作成
#### 5.1.3 KerasによるXORゲートの実装
- ■モデルの作成
- ■モデルの学習
- ■学習結果の確認
#### 5.1.4 TFLearnによるXORゲートの実装
- ■モデルの作成
- ■モデルの学習
- ■学習結果の確認
### 5.2 フィードフォワードニューラルネットワーク（FFNN）
#### 5.2.1 ニューラルネットワークにおける順方向への伝搬
- ■FFNNに必要な関数群を実装する
- ■入力層→第1層
- ■第1層→第2層
- ■第2層→出力層
- ■第3層
### 5.3 バックプロパゲーションを利用した重みの更新
#### 5.3.1 誤差が逆方向に伝搬される流れを見る
#### 5.3.2 行列の掛け算で誤差逆伝播を一発で計算する
- ■一種の正規化因子である行列式の分母を消してしまう
#### 5.3.3 バックプロパゲーション（誤差逆伝搬法）
- ■出力層の重みを更新する（2乗和誤差の場合）
- ■出力層の重みの更新（シグモイド関数を用いるときの交差エントロピー誤差関数の場合）
- ■出力層の重みの更新（ソフトマックス関数を用いるときの交差エントロピー誤差関数の場合）
- ■出力層から1つ手前の層の重みを更新する
- ■重みの更新式を一般化する
### 5.4 ニューラルネットワークの作成
#### 5.4.1 作成するニューラルネットワークの構造
#### 5.4.2 初期化メソッド__init__()、weight_initializer()、sigmoid()、softmax()の作成
- ■重みの初期値について考える
#### 5.4.3 入力層の処理
#### 5.4.4 隠れ層の処理
#### 5.4.5 出力層の処理
- ■ソフトマックス関数
#### 5.4.6 ニューラルネットワークの順伝搬部を完成させる
#### 5.4.7 バックプロパゲーションによる重みの更新
- ■出力層の重みの更新
- ■隠れ層の重みの更新
#### 5.4.8 テストデータを評価するevaluate()メソッドの定義
#### 5.4.9 Python版ニューラルネットワークの完成
### 5.5 ファッションアイテムの画像認識
#### 5.5.1 Fashion-MNISTデータセットのダウンロード
#### 5.5.2 Fashion-MNISTデータの前処理
- ■画像データの前処理
- ■正解ラベルの前処理
- COLUMN MNISTデータとFashion-MNISTデータ
#### 5.5.3 ニューラルネットワークでファッションアイテムの学習を行う
#### 5.5.4 ニューラルネットワークの学習精度を検証する
### 5.6 TensorFlowによるニューラルネットワークの構築
#### 5.6.1 Fashion-MNISTデータセットの用意
- ■Fashion-MNISTデータセットの構造を見る
- ■テスト用のデータを抽出しておく
#### 5.6.2 ブラウザベースで動作するTensorBoard
- ■TensorBoardによる可視化の手順
#### 5.6.3 2層ニューラルネットワークでファッションアイテムの画像を認識する
- ■入力層の作成
- ■隠れ層（第1層）の作成
- ■出力層（第2層）の作成
- ■データフローグラフを完成させる
- ■イベントログをマージするopノード
- ■セッション部の作成
- ■TensorBoardで結果を確認
### 5.7 Keras／TFLearnによるニューラルネットワークの構築
#### 5.7.1 Kerasを用いたニューラルネットワークの構築
- ■Fashion-MNISTデータセットの読み込みと加工
- ■Kerasによるニューラルネットワークの実装
- COLUMN 確率的勾配降下法とミニバッチ法
- ■プログラムの完成
- ■学習を実行し、テストデータで検証する
- ■損失、正解率をグラフにする
- ■ReLU関数
#### 5.7.2 TFLearnを使ってニューラルネットワークを構築する
#### 5.7.3 手書き数字のMNISTデータセットの学習
- ■MNISTデータセットのダウンロードとデータの前処理
- ■MNISTデータの前処理
- ■ニューラルネットワークでMNISTの手書き数字を学習する

## 6章 画像認識のためのディープラーニング
### 6.1 ニューラルネットワークに「特徴検出器」を導入する
- （畳み込みニューラルネットワーク）
#### 6.1.1 2次元フィルターで画像の特徴を検出する
- ■2次元フィルター
- ■2次元フィルターで手書き数字のエッジを抽出してみる
#### 6.1.2 サイズ減した画像をゼロパディングで元のサイズに戻す
#### 6.1.3 Kerasによる畳み込みニューラルネットワーク（CNN）の構築
- ■入力層
- ■畳み込み層
- ■Flatten層
- ■出力層
- ■畳み込みニューラルネットワークの構造
- ■畳み込みニューラルネットワーク（CNN）で画像認識を行う
#### 6.1.4 TFLearnによるCNNの構築
- ■ライブラリのインポートとデータセットの読み込み
- ■データの前処理
- ■畳み込みニューラルネットワーク（CNN）の作成
- ■学習の実行
#### 6.1.5 TensorFlowによるCNNの構築
- ■ライブラリのインポート、データの読み込み、各係数の設定
- ■データフローグラフの作成
- ■セッション部の作成
### 6.2 訓練データに過剰に適合するのを避ける
#### 6.2.1 プーリングで歪みやズレによる影響を回避する
#### 6.2.2 ドロップアウトで過剰適合を回避する
#### 6.2.3 プーリング層とドロップアウトを備えた畳み込みネットワークの構築
#### 6.2.4 TensorFlowによるCNNの構築
- ■入力層の作成
- ■第1層：畳み込み層（ニューロン数＝16）
- ■第2層：畳み込み層（ニューロン数＝32）
- ■第3層：プーリング層（ニューロン数＝32）
- ■第4層：畳み込み層（ニューロン数＝64）
- ■第5層：プーリング層（ニューロン数＝64）
- ■ドロップアウト
- ■Flatten層：（ニューロン数＝7×7×64）
- ■第6層（全結合層）：ニューロン数＝128
- ■第7層（出力層）：ニューロン数＝10
- ■データフローグラフの全コード
- ■セッション部の作成
#### 6.2.5 KerasによるCNNの構築
#### 6.2.6 TFLearnによるCNNの構築
- ■データの読み込みと加工、CNNの構築から学習まで

## 7章 一般物体認識のためのディープラーニング
### 7.1 カラー画像を10のカテゴリに分類したCIFAR-10データセット
#### 7.1.1 一般物体認識のデータセット「CIFAR-10」を題材にする
#### 7.1.2 KerasでダウンロードしたCIFAR-10のカラー画像を見る
#### 7.1.3 一般物体認識のためのKeras、TFLearn共通のCNNの構造
#### 7.1.4 Kerasで構築したCNNに飛行機、自動車、イヌ、ネコなどの10種類の画像を
- 認識させてみる
- ■畳み込みネットワークの構築
- ■CIFAR-10の大量の画像を学習させる
- ■誤差と精度をグラフにする
#### 7.1.6 TFLearnで構築したCNNを用いて飛行機、自動車、イヌ、ネコ…を認識する
- ■CIFAR-10データセットのダウンロード
- ■これまでと同じ構造の畳み込みニューラルネットワークを構築する
- ■TFLearnで構築したCNNを用いてCIFAR-10を学習する
### 7.2 カラー画像に移動、回転などの処理を加えてデータの水増しを行い
- 認識精度を90%に引き上げる
#### 7.2.1 データのスケールを小さくして処理時間をできるだけ短縮させる
#### 7.2.2 訓練データに過剰に適合してしまうのを避ける
- ■出力層の場合の正則化項を適用した重みの更新式
#### 7.2.4 KerasによるCNNの作成
#### 7.2.5 訓練用の画像データを水増しして認識精度を引き上げる
#### 7.2.6 画像をランダムに回転させる
- ■画像を平行に移動する
- ■画像を垂直方向に移動する
- ■画像をランダムに拡大
- ■画像を左右反転
- ■画像を上下反転
- ■画像の色相をランダムに変化させる
#### 7.2.6 画像を拡張処理して精度90％を達成する
- ■学習を実行する部分の作成
- ■損失と精度の変化をグラフにする
- ■学習結果を保存する
- ■画像を入力して認識させてみる

## 8章 人間と機械のセマンティックギャップをなくす試み
### 8.1 CNNで「特徴認識」に近い二値分類、「Dogs vs. Cats」データセットを学習する
#### 8.1.1 Dogs vs. Cats
- ■プログラムでデータセットを振り分ける
#### 8.1.2 KerasでCNNを構築してイヌとネコを認識させてみる
- ■訓練データとテストデータの前処理
- ■CNNの構築
- ■CNNにイヌとネコの画像を学習させる
### 8.2 「転移学習」でイヌとネコを高精度で分類する
#### 8.2.1 自前のFC層に大規模なデータ学習済みのVGG16を結合する
#### 8.2.2 転移学習を行うプログラムの作成
- ■VGG16を利用した処理を実行する関数を作成し、出力結果を保存する
- ■FC層を構築し、VGG16の出力から学習する関数を作成する
- ■精度と損失の推移をグラフにする
- ■イヌとネコの画像を入力して識別させてみる
#### 8.2.3 ファインチューニングを行って、さらに認識精度を引き上げる
- ■ファインチューニングで認識精度を95％まで上げる

## 9章 ジェネレーティブディープラーニング
### 9.1 現在の学習に過去の情報を取り込む（リカレントニューラルネットワーク）
#### 9.1.1 RNN（リカレントニューラルネットワーク）を理解する
- ■BPTT（Backpropagation Through Time）
#### 9.1.2 LSTM（Long Short Term Memory：超短期記憶）
- ■LSTMのファーストステップ
- ■LSTMのセカンドステップ
- ■LSTMのサードステップ
### 9.2 LSTMを配置したRNNで対話が正しいかどうかを識別する
#### 9.2.1 「雑談対話コーパス」、Janomeライブラリのダウンロード
- ■Janomeのインストール
- ■「雑談対話コーパス」のダウンロード
#### 9.2.2 対話データの抽出と加工
- ■JSONファイルを読み込んで正解ラベルと発話をリストにする
- ■発話テキストを形態素に分解する
- ■単語を出現頻度順の数値に置き換える
- ■訓練データのサイズを揃えて正解ラベルをOne-hot表現に変換する
#### 9.2.3 Kerasで構築したRNN（LSTM）を構築して発話が破錠しているかどうかを学習する
- ■対話データを学習する
#### 9.2.4 TFLearnで構築したRNN（LSTM）を構築して発話が破錠しているかどうかを学習する
### 9.3 LSTMを配置したRNNでFashion-MNISTを学習する
#### 9.3.1 LSTMを配置したRNNで画像認識を行う
- ■LSTMを配置したRNNを構築して時系列データを学習する
#### 9.3.2 TFLearnで構築したRNN（LSTM）で画像認識を行う
- ■LSTMを配置したRNNを構築して時系列データを学習する