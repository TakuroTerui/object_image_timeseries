{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ニューラルネットワーク（多層パーセプトロン）_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHavqtPQwQvfwm6zaDTAYE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorchを使ってニューラルネットワークを構築する"
      ],
      "metadata": {
        "id": "_XSxLFeO0s_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q2DBhXNxoHg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1. データの読み込みと前処理\n",
        "'''\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ダウンロード先のディレクトリ\n",
        "root = './data'\n",
        "\n",
        "# トランスフォーマーオブジェクトを生成\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Tensorオブジェクトに変換\n",
        "    transforms.Normalize((0.5), (0.5)), # 平均0.5、標準偏差0.5に正規化\n",
        "    lambda x: x.view(-1) # データの形状を(28, 28)から(784,)に変換\n",
        "])\n",
        "\n",
        "# 訓練用データの読み込み（60000セット）\n",
        "f_mnist_train = datasets.FashionMNIST(\n",
        "    root=root,\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform\n",
        ")\n",
        "# テスト用データの読み込み（10000セット）\n",
        "f_mnist_test = datasets.FashionMNIST(\n",
        "    root=root,\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "batch_size = 64\n",
        "# 訓練用のデータローダー\n",
        "train_dataloader = DataLoader(\n",
        "    f_mnist_train,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "# テスト用のデータローダー\n",
        "test_dataloader = DataLoader(\n",
        "    f_mnist_test,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# データローダーが返すミニバッチの先頭データの形状を出力\n",
        "for (x, t) in train_dataloader:\n",
        "    print(x.shape)\n",
        "    print(t.shape)\n",
        "    break\n",
        "\n",
        "for (x, t) in test_dataloader:\n",
        "    print(x.shape)\n",
        "    print(t.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2. モデルの定義\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    '''\n",
        "    多層パーセプトロン\n",
        "    Attributes:\n",
        "        l1(Linear): 隠れ層\n",
        "        l2(Linear): 出力層\n",
        "        d1(Dropout): ドロップアウト\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        '''\n",
        "        モデルの初期化を行う\n",
        "        Parameters:\n",
        "            input_dim(int): 入力する1データあたりの値の形状\n",
        "            hidden_dim(int): 隠れ層のユニット数\n",
        "            output_dim(int): 出力層のユニット数\n",
        "        '''\n",
        "        # スーパークラスの__init__()を実行\n",
        "        super().__init__()\n",
        "        # 隠れ層\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # ドロップアウト\n",
        "        self.d1 = nn.Dropout(0.5)\n",
        "        # 出力層\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        MLPの順伝播処理を行う\n",
        "        Parameters:\n",
        "            x(ndarray(float32)): 訓練データまたはテストデータ\n",
        "        Returns(float32):\n",
        "            出力層からの出力値\n",
        "        '''\n",
        "        # レイヤー、活性化関数に前ユニットからの出力を入力する\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "y1q6rCvP0gEL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3. モデルの生成\n",
        "'''\n",
        "# 使用可能なデバイスを取得する\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルオブジェクトを生成し、使用可能なデバイスを設定する\n",
        "model = MLP(784, 256, 10).to(device)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC8C2AY62S1R",
        "outputId": "9545f01a-6e75-4264-db36-be9c71d7580c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (d1): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4. 損失関数とオプティマイザーの生成\n",
        "'''\n",
        "import torch.optim\n",
        "\n",
        "# クロスエントロピー誤差のオブジェクトを生成\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 勾配降下アルゴリズムを使用するオプティマイザーを生成\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "bQ8KxeVM2oaS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "5. train_step()関数の定義\n",
        "'''\n",
        "def train_step(x, t):\n",
        "    '''\n",
        "    バックプロパゲーションによるパラメータ更新を行う\n",
        "    Parameters:\n",
        "        x: 訓練データ\n",
        "        t: 正解ラベル\n",
        "    Returns:\n",
        "        MLPの出力と正解ラベルのクロスエントロピー誤差\n",
        "    '''\n",
        "    model.train()\n",
        "    preds = model(x)\n",
        "    loss = criterion(preds, t)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, preds"
      ],
      "metadata": {
        "id": "iXYeNCY03W34"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. test_step()関数の定義\n",
        "'''\n",
        "def test_step(x, t):\n",
        "    '''\n",
        "    テストデータを入力して損失と予測値を返す\n",
        "    Paramters:\n",
        "        x: テストデータ\n",
        "        t: 正解ラベル\n",
        "    Returns:\n",
        "        MLPの出力と正解ラベルのクロスエントロピー誤差\n",
        "    '''\n",
        "    model.eval()\n",
        "    preds = model(x)\n",
        "    loss = criterion(preds, t)\n",
        "\n",
        "    return loss, preds"
      ],
      "metadata": {
        "id": "CUto649L39u_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. 学習の進捗を監視し早期終了判定を行うクラス\n",
        "'''\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=1.0, verbose=0):\n",
        "        '''\n",
        "        Parameters:\n",
        "            patience(int): 監視するエポック数\n",
        "            verbose(int): 早期終了メッセージの出力フラグ\n",
        "        '''\n",
        "        # インスタンス変数の初期化\n",
        "        # 監視中のエポック数のカウンターを初期化\n",
        "        self.epoch = 0\n",
        "        # 比較対象の損失を無限大'inf''で初期化\n",
        "        self.pre_loss = float('inf')\n",
        "        # 監視対象のエポック数をパラメータで初期化\n",
        "        self.patience = patience\n",
        "        # 早期終了メッセージの出力フラグをパラメータで初期化\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def __call__(self, current_loss):\n",
        "        '''\n",
        "        Parameters:\n",
        "            current_loss(float): 1エポック終了後の検証データの損失\n",
        "        Returns:\n",
        "            True: 監視回数の上限までに前エポックの損失を超えた場合\n",
        "            False: 監視回数の上限までに前エポックの損失を超えない場合\n",
        "        '''\n",
        "        # 前エポックの損失より大きくなった場合\n",
        "        if self.pre_loss < current_loss:\n",
        "            self.epoch += 1\n",
        "            # 監視回数の上限に達した場合\n",
        "            if self.epoch > self.patience:\n",
        "                if self.verbose:\n",
        "                    print('early stoopping')\n",
        "                return True\n",
        "        # 前エポックの損失以下の場合\n",
        "        else:\n",
        "            self.epoch = 0\n",
        "            self.pre_loss = current_loss\n",
        "        \n",
        "        return False"
      ],
      "metadata": {
        "id": "_FEJmUSM4dRN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "'''\n",
        "8. モデルを使用して学習する\n",
        "'''\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# エポック数\n",
        "epochs = 200\n",
        "# 損失と精度の履歴を保存するためにdictオブジェクト\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'test_loss': [],\n",
        "    'test_accuracy': [],\n",
        "}\n",
        "# 早期終了の判定を行うオブジェクトを生成\n",
        "ers = EarlyStopping(patience=5, verbose=1)\n",
        "# 学習を行う\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.\n",
        "    train_acc = 0.\n",
        "    test_loss = 0.\n",
        "    test_acc = 0.\n",
        "\n",
        "    # 1ステップにおける訓練用ミニバッチを使用した学習\n",
        "    for (x, t) in train_dataloader:\n",
        "        # torch.Tensorオブジェクトにデバイス割り当てる\n",
        "        x, t = x.to(device), t.to(device)\n",
        "        loss, preds = train_step(x, t)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_score(\n",
        "            t.tolist(),\n",
        "            preds.argmax(dim=-1).tolist()\n",
        "        )\n",
        "\n",
        "    # 1ステップにおけるテストミニバッチを使用した学習\n",
        "    for (x, t) in test_dataloader:\n",
        "        # torch.Tensorオブジェクトにデバイス割り当てる\n",
        "        x, t = x.to(device), t.to(device)\n",
        "        loss, preds = test_step(x, t)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_score(\n",
        "            t.tolist(),\n",
        "            preds.argmax(dim=-1).tolist()\n",
        "        )\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_acc / len(train_dataloader)\n",
        "    avg_test_loss = test_loss / len(test_dataloader)\n",
        "    avg_test_acc = test_acc / len(test_dataloader)\n",
        "\n",
        "    # 訓練データの履歴を保存する\n",
        "    history['loss'].append(avg_train_loss)\n",
        "    history['accuracy'].append(avg_train_acc)\n",
        "    # テストデータの履歴を保存する\n",
        "    history['test_loss'].append(avg_test_loss)\n",
        "    history['test_accuracy'].append(avg_test_acc)\n",
        "\n",
        "    # 1エポックごとに結果を出力\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(\n",
        "            'epoch({}) train_loss: {:.4} train_acc: {:.4} val_loss: {:.4} val_loss: {:.4}'\n",
        "            .format(\n",
        "                epoch + 1,\n",
        "                avg_train_loss,\n",
        "                avg_train_acc,\n",
        "                avg_test_loss,\n",
        "                avg_test_acc,\n",
        "            )\n",
        "        )\n",
        "    # テストデータの損失をEarlyStoppingオブジェクトに渡して早期終了を判定\n",
        "    if ers(avg_test_loss):\n",
        "        break"
      ],
      "metadata": {
        "id": "BMWVQTe16XQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. 損失と精度の推移をグラフにする\n",
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 損失\n",
        "plt.plot(\n",
        "    history['loss'],\n",
        "    marker='.',\n",
        "    label='loss (Training)'\n",
        ")\n",
        "plt.plot(\n",
        "    history['test_loss'],\n",
        "    marker='.',\n",
        "    label='loss (Test)'\n",
        ")\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "# 精度\n",
        "plt.plot(\n",
        "    history['accuracy'],\n",
        "    marker='.',\n",
        "    label='accuracy (Training)'\n",
        ")\n",
        "plt.plot(\n",
        "    history['test_accuracy'],\n",
        "    marker='.',\n",
        "    label='accuracy (Test)'\n",
        ")\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-OQKotCP9y87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KvbFt7Py-nqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}